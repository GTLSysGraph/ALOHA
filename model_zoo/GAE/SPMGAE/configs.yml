cora:
  seeds: [3,1,2,4]
  lr: 0.001
  lr_f: 0.01
  batch_size: 500
  num_heads: 4
  num_layers: 2
  weight_decay: 2e-4
  weight_decay_f: 1e-4
  max_epoch: 1500
  max_epoch_f: 300
  mask_rate: 0.5
  num_layers: 2
  encoder: gat
  decoder: gat
  activation: prelu
  in_drop: 0.2
  attn_drop: 0.1
  linear_prob: True
  loss_fn: sce 
  optimizer: adam
  replace_rate: 0.05
  alpha_l: 3
  scheduler: True
  concat_hidden: False 
  # cora 0.8478±0.0045 ## 0.8400 0.8490 0.8510 0.8510
  # num_hidden         : 512 # 在扰动小的情况下希望大一些而且希望使用graph perb进行重建，扰动小的时候其实所有的边都是有用的，所以其实graph perb比refine多了更多的有用信息来帮助重建，因为加的边其实并不是扰动边
  # drop_edge_rate     : 0.1
  # sim_mode           : jaccard
  # keep_threshold     : 0.02
  # dele_threshold     : 0.02
  # edge_hidden        : 64
  # updata_guide_model : 100
  # decay              : 0.6
  # undirected         : False
  # add_rate           : 0.4
  # gamma              : 10
  # beta               : 0.3

  #### attack cora meta 0.2   0.6457±0.0109
  num_hidden         : 64
  drop_edge_rate     : 0.0
  sim_mode           : jaccard
  keep_threshold     : 0.08
  dele_threshold     : 0.02
  updata_guide_model : 200   # 不能太频繁 100 在攻击的时候不错
  decay              : 0.995 # 在被攻击的时候好像希望新的参数的比重稍微大一些，即这个值稍小一些
  undirected         : False
  add_rate           : 0.2
  gamma              : 1
  beta               : 0.001



citeseer:
  seeds  : [4,1,2,3]
  lr: 0.001
  lr_f: 0.01
  num_hidden: 512
  num_heads: 2
  num_layers: 2
  weight_decay: 2e-5
  weight_decay_f: 0.01
  max_epoch: 300
  max_epoch_f: 300
  mask_rate: 0.5
  num_layers: 2
  encoder: gat
  decoder: gat
  activation: prelu
  in_drop: 0.2  
  attn_drop: 0.1
  linear_prob: True
  loss_fn: sce
  optimizer: adam
  replace_rate: 0.05
  alpha_l: 1 # or 3 
  scheduler: True
  # Citeseer 0.7348±0.0029
  # num_hidden: 512
  # drop_edge_rate     : 0.2
  # sim_mode           : jaccard
  # keep_threshold     : 0.02
  # dele_threshold     : 0.02
  # edge_hidden        : 16
  # updata_guide_model : 200
  # decay              : 0.998
  # undirected         : False
  # add_rate           : 0.5
  # gamma              : 10
  # beta               : 0.01

  #### attack citeseer meta 0.2   
  num_hidden         : 64
  drop_edge_rate     : 0.0
  sim_mode           : jaccard
  keep_threshold     : 0.08
  dele_threshold     : 0.02
  updata_guide_model : 200   # 不能太频繁 100 在攻击的时候不错
  decay              : 0.995 # 在被攻击的时候好像希望新的参数的比重稍微大一些，即这个值稍小一些
  undirected         : False
  add_rate           : 0.2
  gamma              : 1
  beta               : 0.001





pubmed:
  lr: 0.001
  lr_f: 0.01
  num_heads: 4
  num_layers: 2
  weight_decay: 1e-5
  weight_decay_f: 1e-4
  max_epoch: 1000
  max_epoch_f: 300
  mask_rate: 0.75
  num_layers: 2
  encoder: gat
  decoder: gat
  activation: prelu
  in_drop: 0.2
  attn_drop: 0.1
  linear_prob: True
  loss_fn: sce
  drop_edge_rate: 0.0
  optimizer: adam
  replace_rate: 0.0
  alpha_l: 3
  scheduler: True

  #### attack pubmed meta 0.2   
  num_hidden         : 1024
  drop_edge_rate     : 0.0
  sim_mode           : jaccard
  keep_threshold     : 0.08
  dele_threshold     : 0.02
  updata_guide_model : 200   # 不能太频繁 100 在攻击的时候不错
  decay              : 0.995 # 在被攻击的时候好像希望新的参数的比重稍微大一些，即这个值稍小一些
  undirected         : False
  add_rate           : 0.2
  gamma              : 1
  beta               : 0.001


ogbn-arxiv:
  lr: 0.001
  lr_f: 0.01
  num_hidden: 1024
  num_heads: 4
  weight_decay: 0
  weight_decay_f: 5e-4
  max_epoch: 1000
  max_epoch_f: 600
  mask_rate: 0.5
  drop_edge_rate: 0.5
  num_layers: 3
  encoder: gat
  decoder: gat 
  activation: prelu
  in_drop: 0.2
  attn_drop: 0.1
  linear_prob: True
  loss_fn: sce
  replace_rate: 0.0
  alpha_l: 3 # 2
  scheduler: True
  norm: layernorm

ppi:
  lr: 0.0001 # 1e-4 for hidden=1024/2048, 2e-5 for hidden=8192, 5e-6 for hidden=16384
  lr_f: 0.005
  num_hidden: 1024
  num_heads: 4
  weight_decay: 0
  weight_decay_f: 0
  max_epoch: 1000
  max_epoch_f: 2000
  mask_rate: 0.5
  num_layers: 3
  encoder: gat
  decoder: gat
  activation: prelu
  in_drop: 0.2
  loss_fn: sce
  optimizer: adam
  replace_rate: 0.1 # 0.1 for 256*4, else 0. 
  drop_edge_rate: 0.0
  alpha_l: 3
  norm: layernorm
  residual: True
  scheduler: True
  linear_prob: True
reddit:
  lr: 0.001
  lr_f: 0.005
  num_hidden: 512
  num_heads: 2
  weight_decay: 2e-4
  weight_decay_f: 0
  max_epoch: 1
  max_epoch_f: 500
  mask_rate: 0.75
  num_layers: 4
  encoder: gat
  decoder: gat
  activation: prelu
  in_drop: 0.2
  loss_fn: sce
  optimizer: adam
  replace_rate: 0.15
  drop_edge_rate: 0.5
  alpha_l: 3
  norm: layernorm
  residual: True
  scheduler: True
  linear_prob: True
IMDB-BINARY:
  seeds: [1,2,3,4]
  lr: 0.00015
  lr_f: 0.005
  num_hidden: 512
  num_heads: 2
  weight_decay: 0
  weight_decay_f: 0
  max_epoch: 60
  max_epoch_f: 500
  mask_rate: 0.5
  num_layers: 2
  encoder: gin
  decoder: gin
  activation: prelu
  in_drop: 0.2
  loss_fn: sce
  optimizer: adam
  replace_rate: 0.0
  drop_edge_rate: 0.0
  alpha_l: 1
  norm: batchnorm
  residual: False
  scheduler: False
  linear_prob: True
  pooling: mean
  batch_size: 32
  alpha_l: 1
  # 0.7560±0.0062 max 76.40

IMDB-MULTI:
  lr: 0.00015
  num_hidden: 512
  num_heads: 2
  weight_decay: 0
  max_epoch: 50
  mask_rate: 0.5
  num_layers: 3
  encoder: gin
  decoder: gin
  activation: prelu
  in_drop: 0.2
  loss_fn: sce
  optimizer: adam
  replace_rate: 0.0
  drop_edge_rate: 0.0
  alpha_l: 1
  norm: batchnorm
  scheduler: False
  linear_prob: True
  pooling: mean
  batch_size: 32
  alpha_l: 1
PROTEINS:
  lr: 0.00015
  num_hidden: 512
  weight_decay: 0
  max_epoch: 100
  mask_rate: 0.5
  num_layers: 3
  encoder: gin
  decoder: gin
  activation: prelu
  in_drop: 0.2
  loss_fn: sce
  optimizer: adam
  drop_edge_rate: 0.0
  alpha_l: 1
  norm: batchnorm
  scheduler: False
  linear_prob: True
  pooling: max
  batch_size: 32
  norm: batchnorm
  alpha_l: 1
MUTAG:
  seeds: [1,2,3,4]
  num_hidden: 32
  num_layers: 5
  lr: 0.0005
  weight_decay: 0.00
  mask_rate: 0.75
  max_epoch: 20
  encoder: gin
  decoder: gin
  activation: prelu
  loss_fn: sce
  scheduler: False  
  pooling: sum
  batch_size: 64
  alpha_l: 2
  replace_rate: 0.1
  norm: batchnorm
  in_drop: 0.2
  attn_drop: 0.1
  alpha_l: 2
  # MUTAG
  drop_edge_rate     : 0.0
  sim_mode           : jaccard
  keep_threshold     : 0.06
  dele_threshold     : 0.06
  edge_hidden        : 64
  updata_guide_model : 100
  decay              : 0.2
  undirected         : False
  add_rate           : 0.2
  gamma              : 1
  beta               : 1




REDDIT-BINARY:
  lr: 0.00015
  weight_decay: 0.0
  max_epoch: 100
  mask_rate: 0.75
  drop_edge_rate: 0.0
  num_hidden: 512
  num_layers: 2
  encoder: gin
  decoder: gin
  activation: prelu
  pooling: sum
  scheduler: True
  batch_size: 8
  replace_rate: 0.1
  norm: layernorm
  loss_fn: sce
  alpha_l: 2
COLLAB:
  lr: 0.00015
  weight_decay: 0.0
  max_epoch: 20
  num_layers: 2
  num_hidden: 256
  mask_rate: 0.75
  drop_edge_rate: 0.0
  activation: relu
  encoder: gin
  decoder: gin
  scheduler: True
  pooling: max
  batch_size: 32
  loss_fn: sce
  norm: batchnorm
  alpha_l: 1
NCI1:
  lr: 0.001
  max_epoch: 300
  num_layers: 2
  num_hidden: 512
  mask_rate: 0.25 # not 0.75
  drop_edge_rate: 0.0
  activation: prelu
  encoder: gin
  decoder: gin
  scheduler: True
  pool: sum
  batch_size: 16
  alpha_l: 2
  replace_rate: 0.1
  norm: batchnorm
  loss_fn: sce
  alpha_l: 2